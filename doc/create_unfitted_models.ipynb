{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Unfitted Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a tool for easily creating multiple models with some basic hyperparameter tuning - these models are saved as joblib files to the `models/unfitted` directory and can later be fit on your own input data using the Rock Predictor pipeline. As an example of the output, we provide a very basic random forest model as a joblib file under `models/unfitted` in the GitHub repository.\n",
    "\n",
    "To use this notebook, you will need to first run the first 2 steps in the training phase of the Rock Predictor pipeline to calculate and save the features to CSV file format, which output to the `data/pipeline` directory. This can be done by running the target: `make data/pipeline/train_features.csv data/pipeline/test_features.csv`.\n",
    "\n",
    "The files you should have created at this point in the `data/pipeline` folder are: \n",
    "\n",
    "* train.csv\n",
    "* train_features.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import dump, load\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('../rock_predictor'))\n",
    "from helpers.model import ColumnSelector\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in files created by pipeline and look at head and dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/pipeline/train.csv\", index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\"../data/pipeline/train_features.csv\")\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_features.shape)\n",
    "df_features.litho_rock_class.dropna(inplace=True)\n",
    "print(df_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate out non-feature columns from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your own customized columns\n",
    "cols_to_exclude = [\"hole_id\", \"exp_rock_type\", \"exp_rock_class\", \"litho_rock_type\",\n",
    "                   \"litho_rock_class\", 'ActualX_mean', 'ActualY_mean']\n",
    "\n",
    "# Separate target and features\n",
    "X = df_features.drop(columns=cols_to_exclude)\n",
    "y = df_features.litho_rock_class # Target column\n",
    "\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update with your own feature column names\n",
    "cols = ['pos_lagOfLag_median', 'pos_lag1_diff_median', 'time_count', \n",
    "        'hvib_std', 'hvib_max', 'hvib_min', 'hvib_sum','hvib_median', 'hvib_10th_quant','hvib_25th_quant', 'hvib_75th_quant', 'hvib_90th_quant','hvib_num_oscillations', \n",
    "        'vvib_std', 'vvib_max', 'vvib_min', 'vvib_sum','vvib_median', 'vvib_10th_quant', 'vvib_25th_quant', 'vvib_75th_quant','vvib_90th_quant', 'vvib_num_oscillations', \n",
    "        'pull_std', 'pull_max','pull_min', 'pull_sum', 'pull_median', 'pull_10th_quant','pull_25th_quant', 'pull_75th_quant', 'pull_90th_quant','pull_prop_max', 'pull_prop_half', 'pull_num_oscillations', \n",
    "        'air_std', 'air_max', 'air_min', 'air_sum', 'air_median', 'air_10th_quant','air_25th_quant', 'air_75th_quant', 'air_90th_quant','air_num_oscillations', \n",
    "        'pos_std', 'pos_max', 'pos_min', 'pos_sum','pos_median', 'pos_10th_quant', 'pos_25th_quant', 'pos_75th_quant','pos_90th_quant', \n",
    "        'depth_std', 'depth_max', 'depth_min', 'depth_sum','depth_median', 'depth_10th_quant', 'depth_25th_quant','depth_75th_quant', 'depth_90th_quant', \n",
    "        'rot_std', 'rot_max', 'rot_min','rot_sum', 'rot_median', 'rot_10th_quant', 'rot_25th_quant','rot_75th_quant', 'rot_90th_quant', 'rot_num_oscillations', \n",
    "        'water_std','water_max', 'water_min', 'water_sum', 'water_median','water_10th_quant', 'water_25th_quant', 'water_75th_quant','water_90th_quant', 'water_prop_zero', \n",
    "        'penetration_rate',\n",
    "        'exp_rock_type_onehot_AMP', 'exp_rock_type_onehot_GN',\n",
    "        'exp_rock_type_onehot_IF', 'exp_rock_type_onehot_LIMO',\n",
    "        'exp_rock_type_onehot_QR', 'exp_rock_type_onehot_SIF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rfc = Pipeline(steps=[\n",
    "    (\"col_selector\", ColumnSelector(cols)),\n",
    "    (\"rfc\", RandomForestClassifier())])\n",
    "\n",
    "param_dist = { \n",
    "    'rfc__n_estimators': range(10,501),\n",
    "    'rfc__max_features': ['sqrt', 'log2'],\n",
    "    'rfc__max_depth' : range(2,31),\n",
    "    'rfc__criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "cv_rfc = RandomizedSearchCV(estimator=pipe_rfc, param_distributions=param_dist, n_iter=10, cv=5, iid=False)\n",
    "# We're fitting just to find the best hyperparameters, but it will be overwritten in the pipeline\n",
    "cv_rfc.fit(X, y)\n",
    "\n",
    "best_rfc = cv_rfc.best_estimator_\n",
    "best_rfc.description = \"Simple Random Forest\"\n",
    "\n",
    "dump(best_rfc, \"../models/unfitted/randomforest.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_xgb = Pipeline(steps=[\n",
    "    (\"col_selector\", ColumnSelector(cols)),\n",
    "    (\"xgb\", xgb.XGBClassifier())])\n",
    "\n",
    "param_xgb = {\n",
    "        'xgb__learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2, 0.25],\n",
    "        'xgb__max_depth': np.arange(2, 31),\n",
    "        'xgb__n_estimators': np.arange(10, 501, 2),\n",
    "        'xgb__gamma': [0.5, 1, 1.5, 2, 5, 10],\n",
    "        'xgb__reg_lambda': [0.1, 1, 25, 50, 100],\n",
    "        'xgb__reg_alpha': [0, 0.1, 1, 25, 50, 100],\n",
    "        'xgb__min_child_weight': [1, 5, 10],\n",
    "        'xgb__subsample': [0.25, 0.5, 0.6, 0.8, 1.0],\n",
    "        'xgb__colsample_bytree': [0.5, 0.8, 1.0],\n",
    "        }\n",
    "\n",
    "cv_xgb = RandomizedSearchCV(estimator=pipe_xgb, param_distributions=param_xgb, n_iter=10, cv=5, iid=False)\n",
    "# We're fitting just to find the best hyperparameters, but it will be overwritten in the pipeline\n",
    "cv_xgb.fit(X, y)\n",
    "\n",
    "best_xgb = cv_xgb.best_estimator_\n",
    "best_xgb.description = \"Simple XGBoost\"\n",
    "\n",
    "dump(best_xgb, \"../models/unfitted/xgboost.joblib\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
